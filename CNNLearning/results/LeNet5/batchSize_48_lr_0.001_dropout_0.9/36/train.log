Early stopping: patience = 5, val_interval = 10, delta = 0.001
==> use gpu id: 0
==> Obtain supervised dataset
train dataset size (samples × weight × height × channel): (40000, 3, 32, 32)
valid dataset size (samples × weight × height × channel): (10000, 3, 32, 32)
test dataset size (samples × weight × height × channel): (10000, 3, 32, 32)
labels name: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

==> Build network
network structure: LeNet5(
  (C1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU()
  )
  (S2): Sequential(
    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (C3): Sequential(
    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU()
  )
  (S4): Sequential(
    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (C5): Sequential(
    (0): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU()
  )
  (F6): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=120, out_features=84, bias=True)
    (2): Linear(in_features=84, out_features=10, bias=True)
  )
  (LeNet5): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
      (1): ReLU()
    )
    (1): Sequential(
      (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
      (1): ReLU()
    )
    (3): Sequential(
      (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (4): Sequential(
      (0): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))
      (1): ReLU()
    )
    (5): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=120, out_features=84, bias=True)
      (2): Linear(in_features=84, out_features=10, bias=True)
    )
  )
)
==> Build loss and optimizer
==> Training with validation dataset
Epoch: 1; loss: 1.7739291835746034
Epoch: 1; val_precision: 0.4189
Epoch: 2; loss: 1.532050453215766
Epoch: 2; val_precision: 0.4466
Epoch: 3; loss: 1.4204347999118787
Epoch: 3; val_precision: 0.4791
Epoch: 4; loss: 1.3423003700854395
Epoch: 4; val_precision: 0.4614
Epoch: 5; loss: 1.2638814734469215
Epoch: 5; val_precision: 0.5043
Epoch: 6; loss: 1.2035990512485413
Epoch: 6; val_precision: 0.5242
Epoch: 7; loss: 1.1442915997345098
Epoch: 7; val_precision: 0.5441
Epoch: 8; loss: 1.0978459648662906
Epoch: 8; val_precision: 0.5303
Epoch: 9; loss: 1.0490730057517401
Epoch: 9; val_precision: 0.5319
Epoch: 10; loss: 0.9959057661698019
Epoch: 10; val_precision: 0.5439
Epoch: 11; loss: 0.968803506818035
Epoch: 11; val_precision: 0.5256
Epoch: 12; loss: 0.9336641485528122
Epoch: 12; val_precision: 0.5177
Epoch: 13; loss: 0.8983117358075629
Epoch: 13; val_precision: 0.5307
Epoch: 14; loss: 0.8679637775646982
Epoch: 14; val_precision: 0.5126
Epoch: 15; loss: 0.8367067357118753
Epoch: 15; val_precision: 0.5289
Epoch: 16; loss: 0.8087529775192983
Epoch: 16; val_precision: 0.5198
Epoch: 17; loss: 0.7929089623961232
Epoch: 17; val_precision: 0.5254
Epoch: 18; loss: 0.7477699085462579
Epoch: 18; val_precision: 0.518
Epoch: 19; loss: 0.7370670555735663
Epoch: 19; val_precision: 0.52
Epoch: 20; loss: 0.7284262324337193
Epoch: 20; val_precision: 0.5121
Epoch: 21; loss: 0.695866224755772
Epoch: 21; val_precision: 0.5088
Epoch: 22; loss: 0.685211824838349
Epoch: 22; val_precision: 0.5102
Epoch: 23; loss: 0.6588823057407384
Epoch: 23; val_precision: 0.505
Epoch: 24; loss: 0.6452302429601728
Epoch: 24; val_precision: 0.5115
Epoch: 25; loss: 0.6341014080446401
Epoch: 25; val_precision: 0.5093
Epoch: 26; loss: 0.6082926660561733
Epoch: 26; val_precision: 0.5056
Epoch: 27; loss: 0.6099960371065769
Epoch: 27; val_precision: 0.5101
Epoch: 28; loss: 0.5786855465395273
Epoch: 28; val_precision: 0.4988
Epoch: 29; loss: 0.5712824820650282
Epoch: 29; val_precision: 0.5052
Epoch: 30; loss: 0.5619849310540181
Epoch: 30; val_precision: 0.4991
Epoch: 31; loss: 0.5421288737433134
Epoch: 31; val_precision: 0.4966
Epoch: 32; loss: 0.54299304373473
Epoch: 32; val_precision: 0.4952
Epoch: 33; loss: 0.5318432380255463
Epoch: 33; val_precision: 0.5011
Epoch: 34; loss: 0.519513716193126
Epoch: 34; val_precision: 0.4995
Epoch: 35; loss: 0.5199545876835462
Epoch: 35; val_precision: 0.4887
Epoch: 36; loss: 0.4962544972054678
Epoch: 36; val_precision: 0.4875
Epoch: 37; loss: 0.4963642368964154
Epoch: 37; val_precision: 0.4941
Epoch: 38; loss: 0.49025604809574086
Epoch: 38; val_precision: 0.4939
Epoch: 39; loss: 0.4814426733149613
Epoch: 39; val_precision: 0.4968
Epoch: 40; loss: 0.47110849449174297
Epoch: 40; val_precision: 0.495
Epoch: 41; loss: 0.45533937112175876
Epoch: 41; val_precision: 0.4873
Epoch: 42; loss: 0.4686803871797
Epoch: 42; val_precision: 0.4871
Epoch: 43; loss: 0.4502669765794878
Epoch: 43; val_precision: 0.4779
Epoch: 44; loss: 0.4619761622852559
Epoch: 44; val_precision: 0.4804
Epoch: 45; loss: 0.4286264664799475
Epoch: 45; val_precision: 0.4874
Epoch: 46; loss: 0.4296303112908519
Epoch: 46; val_precision: 0.4836
Epoch: 47; loss: 0.4258837100984953
Epoch: 47; val_precision: 0.4832
Epoch: 48; loss: 0.43345182994715603
Epoch: 48; val_precision: 0.4883
Epoch: 49; loss: 0.40542371664866267
Epoch: 49; val_precision: 0.4828
Epoch: 50; loss: 0.4145499462757608
Epoch: 50; val_precision: 0.4819
Epoch: 51; loss: 0.4020058677601014
Epoch: 51; val_precision: 0.4764
Epoch: 52; loss: 0.3969393492602616
Epoch: 52; val_precision: 0.4822
Epoch: 53; loss: 0.382378688895052
Epoch: 53; val_precision: 0.4809
Epoch: 54; loss: 0.3858278421305424
Epoch: 54; val_precision: 0.4885
Epoch: 55; loss: 0.3916710932567108
Epoch: 55; val_precision: 0.4789
Epoch: 56; loss: 0.39588042622883257
Epoch: 56; val_precision: 0.4816
Epoch: 57; loss: 0.36474241381980105
Epoch: 57; val_precision: 0.4803
Epoch: 58; loss: 0.3788030870634017
Epoch: 58; val_precision: 0.4804
Epoch: 59; loss: 0.3658111060587622
Epoch: 59; val_precision: 0.4858
Epoch: 60; loss: 0.35502646566866686
Epoch: 60; val_precision: 0.484
Epoch: 61; loss: 0.35991443959857633
Epoch: 61; val_precision: 0.4798
Epoch: 62; loss: 0.35973782661399967
Epoch: 62; val_precision: 0.4724
Epoch: 63; loss: 0.3397092312056241
Epoch: 63; val_precision: 0.4828
Epoch: 64; loss: 0.3575431463225997
Epoch: 64; val_precision: 0.4755
Epoch: 65; loss: 0.3412746868857877
Epoch: 65; val_precision: 0.4744
Epoch: 66; loss: 0.3431422300726104
Epoch: 66; val_precision: 0.4786
Epoch: 67; loss: 0.3279470539957785
Epoch: 67; val_precision: 0.48
Epoch: 68; loss: 0.3534660323989763
Epoch: 68; val_precision: 0.4736
Epoch: 69; loss: 0.33369946510171433
Epoch: 69; val_precision: 0.4725
Epoch: 70; loss: 0.3164498840751265
Epoch: 70; val_precision: 0.4705
==> Early stopping!
==> Best epoch has been learned, which is 20
==> Combine train and valid dataset
==> Build new network
==> Build new loss and optimizer
==> Training with combined training dataset
Epoch: 10; loss: 0.9391937703752243
Epoch: 20; loss: 0.7284088788014227
==> Training complete!
